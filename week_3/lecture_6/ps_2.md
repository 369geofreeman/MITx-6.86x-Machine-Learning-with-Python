# Problem set 2


## Introduction to Non-linear Classification

### Counting Dimensions of Feature Vectorsi



Let  ğ‘¥âˆˆğ‘150,  i.e.  ğ‘¥=[ğ‘¥1,ğ‘¥2,...,ğ‘¥150]ğ‘‡  where  ğ‘¥ğ‘–  is the  ğ‘– -th component of  ğ‘¥ . Let  ğœ™(ğ‘¥)  be an order  3  polynomial feature vector. This means, for example,  ğœ™(ğ‘¥)  can be

 	 ğœ™(ğ‘¥)=[ğ‘¥1,...,ğ‘¥ğ‘–,...,ğ‘¥150î„½î„¾î…î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹deg 1,ğ‘¥21,ğ‘¥1ğ‘¥2,...,ğ‘¥ğ‘–ğ‘¥ğ‘—,...ğ‘¥1502î„½î„¾î…î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹deg 2,ğ‘¥13,ğ‘¥12ğ‘¥2,...,ğ‘¥ğ‘–ğ‘¥ğ‘—ğ‘¥ğ‘˜,...,ğ‘¥1503î„½î„¾î…î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹î…‹deg 3]where 1â‰¤ğ‘–â‰¤ğ‘—â‰¤ğ‘˜â‰¤150. 	 	 
Note that the components of  ğœ™(ğ‘¥)  forms a basis of the space of all polynomials with zero constant term and of degree at most  3 .

What is the dimension of the space that  ğœ™(ğ‘¥)  lives in? That is,  ğœ™(ğ‘¥)âˆˆâ„ğ‘‘  for what  ğ‘‘ ?

Hint: The number of ways to select a multiset of  ğ‘˜  non-unique items from  ğ‘›  total is  (ğ‘›+ğ‘˜âˆ’1ğ‘˜) . For example, if a ball can be any of 3 colors, then the number of color configurations of 2 balls is  (3+2âˆ’12)=(42)=6 .


Answer:  d = 585275

Solution:

For each of the feature transformations (power 1, power 2, power 3), there are n-multichoose-power combinations. Thus  (1501)+(1512)+(1523)=585275 . Remark: We see that the dimension of the space that the feature vectors live grows quickly as a function of  ğ‘‘,  the dimension we started with if  ğ‘¥âˆˆâ„ğ‘‘ . 


### Regression using Higher Order Polynomial feature

Assume we have  ğ‘›  data points in the training set  {(ğ‘¥(ğ‘¡),ğ‘¦(ğ‘¡))}ğ‘¡=1,...,ğ‘›  where  (ğ‘¥(ğ‘¡),ğ‘¦(ğ‘¡))  is the  ğ‘¡ -th training example:


We want to find a non-linear regression function  ğ‘“  that predicts  ğ‘¦  from  ğ‘¥ , given by

ğ‘“(ğ‘¥;ğœƒ,ğœƒ0)=ğœƒâ‹…ğœ™(ğ‘¥)+ğœƒ0 
 
where  ğœ™(ğ‘¥)  is a polynomial feature vector of some order. What (loosely) is the minimum order of  ğœ™(ğ‘¥) ?

Answer = 3

Solution:

The relationship between  ğ‘¦  and  ğ‘¥  can be roughly described by a cubic function, so a feature vector  ğœ™(ğ‘¥)  of minimum order 3 can minimize structural errors.


### Effect of Regularization on Higher Order Regression


Let us go back to explore the effect of regularizaion on Higher Order regression.

The three figures below show the fitting result of a 9th order polynomial regression with different regularization parameter  ğœ†  on the same training data.

 â€ƒâ€ƒ â€ƒâ€ƒ

Which figure above corresponds to the smallest regularization parameter  ğœ† ?

Answer = A


Which figure corresponds to the largest regularization parameter  ğœ† ?

Answer = B


Solution:

The effect of regularization is to restrict the parameters of a model to freely take on large values. This will make the model function smoother, leveling the 'hills' and filling the 'vallyes'. It will also make the model more stable, as a small perturbation on  ğ‘¥  will not change  ğ‘¦  significantly with smaller  â€–ğœƒâ€– .
