# Problem set one


##Â Higher Order Feature Vectors


We can use linear classifiers to make non-linear predictions. The easiest way to do this is to first map all the examples  ğ‘¥âˆˆâ„ğ‘‘  to different feature vectors  ğœ™(ğ‘¥)âˆˆâ„ğ‘  where typically  ğ‘  is much larger than  ğ‘‘ . We would then simply use a linear classifier on the new (higher dimensional) feature vectors, pretending that they were the original input vectors. As a result, all the linear classifiers we have learned remain applicable, yet produce non-linear classifiers in the original coordinates.

There are many ways to create such feature vectors. One common way is to use polynomial terms of the original coordinates as the components of the feature vectors. We have seen two examples in the video above. We will recall the 1-dimensional example here and see another 2-dimensional example in the problem below.

Example: Given 3 training examples with  ğ‘¥(ğ‘¡)âˆˆâ„(ğ‘¡=1,2,3)  that are not linearly separable in 1-dimensional space as shown below,


define the feature map  ğœ™(ğ‘¥)=[ğœ™1(ğ‘¥),ğœ™2(ğ‘¥)]ğ‘‡=[ğ‘¥,ğ‘¥2]ğ‘‡,  which maps each example  ğ‘¥(ğ‘¡)  in 1-dimensional space to a corresponding feature vector  ğœ™(ğ‘¥(ğ‘¡))=[ğ‘¥(ğ‘¡),(ğ‘¥(ğ‘¡))2]ğ‘‡  in 2-dimensional space.

Then, instead of using  (ğ‘¥(ğ‘¡),ğ‘¦(ğ‘¡)),  we use  (ğœ™(ğ‘¥(ğ‘¡)),ğ‘¦(ğ‘¡))  as the training examples (where  ğ‘¦(ğ‘¡)  are the labels):


The new training set is linearly separable in the 2-dimensional  (ğœ™1,ğœ™2) -space, and we can train a â€œlinear classifier" that is linear in the  ğœ™ -coordinates.


## Another 2-Dimensional Example

Given the training examples with  ğ‘¥=[ğ‘¥(ğ‘¡)1,ğ‘¥(ğ‘¡)2]âˆˆâ„2  above, where a boundary between the positively-labeled examples and the negatively-labeled examples is an ellipse , which of the following feature vector(s)  ğœ™(ğ‘¥)  will guarantee that the training set  {(ğœ™(ğ‘¥(ğ‘¡)),ğ‘¦(ğ‘¡)),ğ‘¡=1,â€¦,ğ‘›}  (where  ğ‘¦(ğ‘¡)  are the labels) are linearly separable?
(Choose all that apply.)

Answer = ğœ™(ğ‘¥)=[ğ‘¥1,ğ‘¥2,ğ‘¥1ğ‘¥2,ğ‘¥21,ğ‘¥22]ğ‘‡ 

Solution:

Since a possible boundary is an elipse, and we recall from geometry that the equation of any ellipse can be given as

 	 ğœƒ1ğ‘¥1+ğœƒ2ğ‘¥2+ğœƒ3ğ‘¥1ğ‘¥2+ğœƒ4ğ‘¥21+ğœƒ5ğ‘¥22+ğœƒ0=ğœƒâ‹…[ğ‘¥1,ğ‘¥2,ğ‘¥1ğ‘¥2,ğ‘¥21,ğ‘¥22]ğ‘‡+ğœƒ0=0, 	 	 
for some  ğœƒ=[ğœƒ1,ğœƒ2,ğœƒ3,ğœƒ4,ğœƒ5],  we see that defining the feature map to be  ğœ™(ğ‘¥)=[ğ‘¥1,ğ‘¥2,ğ‘¥1ğ‘¥2,ğ‘¥21,ğ‘¥22]ğ‘‡  (the last choice) will allow us to write a linear decision boundary in the  ğœ™ -coordinates:

 	  	 ğœƒâ‹…ğœ™(ğ‘¥)+ğœƒ0 	 = 	 0âŸº 	 ğœƒâ‹…[ğ‘¥1,ğ‘¥2,ğ‘¥1ğ‘¥2,ğ‘¥21,ğ‘¥22]ğ‘‡+ğœƒ0 	 = 	 0. 	 	 
Let us examine the first three choices in order:

The first choice  ğœ™(ğ‘¥)=[ğ‘¥1,ğ‘¥2]ğ‘‡  maps  ğ‘¥  to  ğ‘¥  itself in  â„2 , so the training set remains the same and not linearly-separable.

The second choice  ğœ™(ğ‘¥)=[ğ‘¥1,ğ‘¥2,ğ‘¥21+ğ‘¥22]ğ‘‡  gives decision boundaries of the form:

 	 ğœƒâ‹…[ğ‘¥1,ğ‘¥2,ğ‘¥21+ğ‘¥22]ğ‘‡+ğœƒ0=ğœƒ1ğ‘¥21+ğœƒ2ğ‘¥22+ğœƒ3(ğ‘¥21+ğ‘¥22)+ğœƒ0=0, 	 	 
which are circles in the  (ğ‘¥1,ğ‘¥2)âˆ’  plane. Hence, if the decision boundary in the  (ğ‘¥1,ğ‘¥2)âˆ’  plane is an eclipse that is not circular, this choice of  ğœ™  will not give a linear decision boundary in the  ğœ™ -coordinates.

The argument for the third choice is analogous to the second choice above.

The  ğ‘¥1ğ‘¥2  term is needed in the fifth solution, since the decision boundary is a rotated eclipse.

