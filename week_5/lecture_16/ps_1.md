# Recap of Maximum Likelihood Estimation for Multinomial and Gaussian Models


So far, in clustering we have assumed that the data has no probabilistic generative model attached to it, and we have used various iterative algorithms based on similarity measures to come up with a way to group similar data points into clusters. In this lecture, we will assume an underlying probabilistic generative model that will lead us to a natural clustering algorithm called the EM algorithm .

While a â€œhard" clustering algorithm like k-means or k-medoids can only provide a cluster ID for each data point, the EM algorithm, along with the generative model driving its equations, can provide the posterior probability (â€œsoft" assignments) that every data point belongs to any cluster.

The EM algorithm will also form the basis for a portion of Project 4 in which we explore collaborative filtering via Gaussian mixtures.



##Â MLE under Gaussian Noise I

Let  ğ‘Œğ‘–=ğœƒ+ğ‘ğ‘–,ğ‘–=1,â€¦,ğ‘› , where  ğœƒ  is an unknown parameter and  ğ‘ğ‘–  are iid Gaussian random variables with zero mean. Upon observing  ğ‘Œğ‘– 's, what is the maximum likelihood estimate of  ğœƒ ?

Choose the correct expression from options below.

Hint: For this problem, think of the distributional property of the random variables  ğ‘Œğ‘–  and how they are derived from  ğ‘ğ‘– 's. Also, remember that  ğ‘ğ‘–  are iid Gaussian random variables.


Answer =  ğœƒÌ‚ =âˆ‘ğ‘›ğ‘–=1ğ‘Œğ‘–ğ‘›  correct 



## MLE under Gaussian Noise II


Would the ML estimator of  ğœƒ  change if the  ğ‘ğ‘– 's are independent Gaussians with possibly different variances  ğœ21,â€¦,ğœ2ğ‘›  but same zero mean? Assume that  ğœ2ğ‘–  are known constants.


Answer = YES


## MLE under Gaussian Noise III

Now, let  ğ‘Œğ‘–=ğœƒ+ğ‘ğ‘–,ğ‘–=1,â€¦,ğ‘› , where  ğœƒ  is an unknown parameter. This time, let  ğ‘ğ‘–  for all  ğ‘–=1,â€¦,ğ‘›  each be a linear combination of  â„“  zero-mean, independent Gaussian random variables with possibly different variances. That is, let  ğ‘ğ‘–=ğ‘ğ‘–,1+â‹¯+ğ‘ğ‘–,â„“ , where  ğ‘ğ‘–,ğ‘—  for a given  ğ‘–  are independent with possibly different variances. Let  {ğ‘ğ‘–,ğ‘—:ğ‘–=1,â€¦,ğ‘› and ğ‘—=1,â€¦,â„“}  be independent random variables, but with the condition that for a given  ğ‘—  the variance of  ğ‘ğ‘–,ğ‘—  is the same for  ğ‘–=1,â€¦,ğ‘› .

Is the ML estimator of  ğœƒ  the same as the one we found in the problem "MLE under Gaussian Noise I"?


Answer = YES




