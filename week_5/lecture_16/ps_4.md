# Mixture Model - Unobserved Case: EM Algorithm


Estimates of Parameters of GMM: The Expectation Maximization (EM) Algorithm

We observe  ğ‘›  data points  ğ±1,â€¦,ğ±ğ‘›  in  â„ğ‘‘ . We wish to maximize the GMM likelihood with respect to the parameter set  ğœƒ={ğ‘1,â€¦,ğ‘ğ¾,ğœ‡(1),â€¦,ğœ‡(ğ¾),ğœ21,â€¦,ğœ2ğ¾} .

Maximizing the log-likelihood  log(âˆğ‘›ğ‘–=1ğ‘(ğ±(ğ‘–)|ğœƒ))  is not tractable in the setting of GMMs. There is no closed-form solution to finding the parameter set  ğœƒ  that maximizes the likelihood. The EM algorithm is an iterative algorithm that finds a locally optimal solution  ğœƒÌ‚   to the GMM likelihood maximization problem.

E Step

The E Step of the algorithm involves finding the posterior probability that point  ğ±(ğ‘–)  was generated by cluster  ğ‘— , for every  ğ‘–=1,â€¦,ğ‘›  and  ğ‘—=1,â€¦,ğ¾ . This step assumes the knowledge of the parameter set  ğœƒ . We find the posterior using the following equation:

 	 ğ‘(point ğ±(ğ‘–) was generated by cluster ğ‘—|ğ±(ğ‘–),ğœƒ)â‰œğ‘(ğ‘—âˆ£ğ‘–)=ğ‘ğ‘—îˆº(ğ±(ğ‘–);ğœ‡(ğ‘—),ğœ2ğ‘—ğ¼)ğ‘(ğ±(ğ‘–)âˆ£ğœƒ). 	 	 
M Step

The M Step of the algorithm maximizes a proxy function  â„“Ì‚ (ğ±(1),â€¦,ğ±(ğ‘›)âˆ£ğœƒ)  of the log-likelihood over  ğœƒ , where

 	 â„“Ì‚ (ğ±(1),â€¦,ğ±(ğ‘›)âˆ£ğœƒ)â‰œâˆ‘ğ‘–=1ğ‘›âˆ‘ğ‘—=1ğ¾ğ‘(ğ‘—âˆ£ğ‘–)log(ğ‘(ğ±(ğ‘–) and ğ±(ğ‘–) generated by cluster ğ‘—âˆ£ğœƒ)ğ‘(ğ‘—âˆ£ğ‘–)). 	 	 
This is done instead of maximizing over  ğœƒ  the actual log-likelihood

 	 â„“(ğ±(1),â€¦,ğ±(ğ‘›)âˆ£ğœƒ)=âˆ‘ğ‘–=1ğ‘›log[âˆ‘ğ‘—=1ğ¾ğ‘(ğ±(ğ‘–) generated by cluster ğ‘—âˆ£ğœƒ)]. 	 	 
Maximizing the proxy function over the parameter set  ğœƒ , one can verify by taking derivatives and setting them equal to zero that

 	 ğœ‡(ğ‘—)Ë† 	 =âˆ‘ğ‘›ğ‘–=1ğ‘(ğ‘—âˆ£ğ‘–)ğ±(ğ‘–)âˆ‘ğ‘›ğ‘–=1ğ‘(ğ‘—âˆ£ğ‘–) 	 	 
 	 ğ‘ğ‘—Ë† 	 =1ğ‘›âˆ‘ğ‘–=1ğ‘›ğ‘(ğ‘—âˆ£ğ‘–), 	 	 
 	 ğœ2ğ‘—Ë† 	 =âˆ‘ğ‘›ğ‘–=1ğ‘(ğ‘—âˆ£ğ‘–)â€–ğ±(ğ‘–)âˆ’ğœ‡(ğ‘—)Ë†â€–2ğ‘‘âˆ‘ğ‘›ğ‘–=1ğ‘(ğ‘—âˆ£ğ‘–). 	 	 
The E and M steps are repeated iteratively until there is no noticeable change in the actual likelihood computed after M step using the newly estimated parameters or if the parameters do not vary by much.

Initialization

As for the initialization before the first time E step is carried out, we can either do a random initialization of the parameter set  ğœƒ  or we can employ k-means to find the initial cluster centers of the  ğ¾  clusters and use the global variance of the dataset as the initial variance of all the  ğ¾  clusters. In the latter case, the mixture weights can be initialized to the proportion of data points in the clusters as found by the k-means algorithm.



## Gaussian Mixture Model: An Example Update - E-Step


Assume that the initial means and variances of two clusters in a GMM are as follows:  ğœ‡(1)=âˆ’3 ,  ğœ‡(2)=2 ,  ğœ21=ğœ22=4 . Let  ğ‘1=ğ‘2=0.5 .

Let  ğ‘¥(1)=0.2 ,  ğ‘¥(2)=âˆ’0.9 ,  ğ‘¥(3)=âˆ’1 ,  ğ‘¥(4)=1.2 ,  ğ‘¥(5)=1.8  be five points that we wish to cluster.

In this problem and in the next, we compute the updated parameters corresponding to cluster 1. You may use any computational tool at your disposal.

Compute the following posterior probabilities (provide at least five decimal digits):

ğ‘(1âˆ£1)= 

ğ‘(1âˆ£2)= 

ğ‘(1âˆ£3)= 

ğ‘(1âˆ£4)= 

ğ‘(1âˆ£5)= 


##Â Gaussian Mixture Model: An Example Update - M-Step



Compute the updated parameters corresponding to cluster 1 (provide at least five decimal digits):


ğ‘Ì‚ 1= 


ğœ‡Ì‚ 1= 

ğœÌ‚ 21=


##Â  Gaussian Mixture Model and the EM Algorithm


Which of the following statements are true? Assume that we have a Gaussian mixture model with known (or estimated) parameters (means and variances of the Gaussians and the mixture weights).


Answer = 
