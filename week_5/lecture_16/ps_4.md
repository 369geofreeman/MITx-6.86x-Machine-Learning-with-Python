# Mixture Model - Unobserved Case: EM Algorithm


Estimates of Parameters of GMM: The Expectation Maximization (EM) Algorithm

We observe  𝑛  data points  𝐱1,…,𝐱𝑛  in  ℝ𝑑 . We wish to maximize the GMM likelihood with respect to the parameter set  𝜃={𝑝1,…,𝑝𝐾,𝜇(1),…,𝜇(𝐾),𝜎21,…,𝜎2𝐾} .

Maximizing the log-likelihood  log(∏𝑛𝑖=1𝑝(𝐱(𝑖)|𝜃))  is not tractable in the setting of GMMs. There is no closed-form solution to finding the parameter set  𝜃  that maximizes the likelihood. The EM algorithm is an iterative algorithm that finds a locally optimal solution  𝜃̂   to the GMM likelihood maximization problem.

E Step

The E Step of the algorithm involves finding the posterior probability that point  𝐱(𝑖)  was generated by cluster  𝑗 , for every  𝑖=1,…,𝑛  and  𝑗=1,…,𝐾 . This step assumes the knowledge of the parameter set  𝜃 . We find the posterior using the following equation:

 	 𝑝(point 𝐱(𝑖) was generated by cluster 𝑗|𝐱(𝑖),𝜃)≜𝑝(𝑗∣𝑖)=𝑝𝑗(𝐱(𝑖);𝜇(𝑗),𝜎2𝑗𝐼)𝑝(𝐱(𝑖)∣𝜃). 	 	 
M Step

The M Step of the algorithm maximizes a proxy function  ℓ̂ (𝐱(1),…,𝐱(𝑛)∣𝜃)  of the log-likelihood over  𝜃 , where

 	 ℓ̂ (𝐱(1),…,𝐱(𝑛)∣𝜃)≜∑𝑖=1𝑛∑𝑗=1𝐾𝑝(𝑗∣𝑖)log(𝑝(𝐱(𝑖) and 𝐱(𝑖) generated by cluster 𝑗∣𝜃)𝑝(𝑗∣𝑖)). 	 	 
This is done instead of maximizing over  𝜃  the actual log-likelihood

 	 ℓ(𝐱(1),…,𝐱(𝑛)∣𝜃)=∑𝑖=1𝑛log[∑𝑗=1𝐾𝑝(𝐱(𝑖) generated by cluster 𝑗∣𝜃)]. 	 	 
Maximizing the proxy function over the parameter set  𝜃 , one can verify by taking derivatives and setting them equal to zero that

 	 𝜇(𝑗)ˆ 	 =∑𝑛𝑖=1𝑝(𝑗∣𝑖)𝐱(𝑖)∑𝑛𝑖=1𝑝(𝑗∣𝑖) 	 	 
 	 𝑝𝑗ˆ 	 =1𝑛∑𝑖=1𝑛𝑝(𝑗∣𝑖), 	 	 
 	 𝜎2𝑗ˆ 	 =∑𝑛𝑖=1𝑝(𝑗∣𝑖)‖𝐱(𝑖)−𝜇(𝑗)ˆ‖2𝑑∑𝑛𝑖=1𝑝(𝑗∣𝑖). 	 	 
The E and M steps are repeated iteratively until there is no noticeable change in the actual likelihood computed after M step using the newly estimated parameters or if the parameters do not vary by much.

Initialization

As for the initialization before the first time E step is carried out, we can either do a random initialization of the parameter set  𝜃  or we can employ k-means to find the initial cluster centers of the  𝐾  clusters and use the global variance of the dataset as the initial variance of all the  𝐾  clusters. In the latter case, the mixture weights can be initialized to the proportion of data points in the clusters as found by the k-means algorithm.



## Gaussian Mixture Model: An Example Update - E-Step


Assume that the initial means and variances of two clusters in a GMM are as follows:  𝜇(1)=−3 ,  𝜇(2)=2 ,  𝜎21=𝜎22=4 . Let  𝑝1=𝑝2=0.5 .

Let  𝑥(1)=0.2 ,  𝑥(2)=−0.9 ,  𝑥(3)=−1 ,  𝑥(4)=1.2 ,  𝑥(5)=1.8  be five points that we wish to cluster.

In this problem and in the next, we compute the updated parameters corresponding to cluster 1. You may use any computational tool at your disposal.

Compute the following posterior probabilities (provide at least five decimal digits):

𝑝(1∣1)= 

𝑝(1∣2)= 

𝑝(1∣3)= 

𝑝(1∣4)= 

𝑝(1∣5)= 


## Gaussian Mixture Model: An Example Update - M-Step



Compute the updated parameters corresponding to cluster 1 (provide at least five decimal digits):


𝑝̂ 1= 


𝜇̂ 1= 

𝜎̂ 21=


##  Gaussian Mixture Model and the EM Algorithm


Which of the following statements are true? Assume that we have a Gaussian mixture model with known (or estimated) parameters (means and variances of the Gaussians and the mixture weights).


Answer = 
