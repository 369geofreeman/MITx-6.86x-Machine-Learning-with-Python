##  EM Algorithm


Consider the following mixture of two Gaussians:

ğ‘(ğ‘¥;ğœƒ)=ğœ‹1îˆº(ğ‘¥;ğœ‡1,ğœ21)+ğœ‹2îˆº(ğ‘¥;ğœ‡2,ğœ22) 
 
This mixture has parameters  ğœƒ={ğœ‹1,ğœ‹2,ğœ‡1,ğœ‡2,ğœ21,ğœ22} . They correspond to the mixing proportions, means, and variances of each Gaussian. We initialize  ğœƒ  as  ğœƒ0={0.5,0.5,6,7,1,4} .

We have a dataset  îˆ°  with the following samples of  ğ‘¥ :  ğ‘¥(0)=âˆ’1 ,  ğ‘¥(1)=0 ,  ğ‘¥(2)=4 ,  ğ‘¥(3)=5 ,  ğ‘¥(4)=6 .

We want to set our parameters  ğœƒ  such that the data log-likelihood  ğ‘™(îˆ°;ğœƒ)  is maximized:

argmaxğœƒ âˆ‘ğ‘–=04logğ‘(ğ‘¥(ğ‘–);ğœƒ). 
 
Recall that we can do this with the EM algorithm. The algorithm optimizes a lower bound on the log-likelihood, thus iteratively pushing the data likelihood upwards. The iterative algorithm is specified by two steps applied successively:

E-step: infer component assignments from current  ğœƒ0=ğœƒ  (complete the data)

ğ‘(ğ‘¦=ğ‘˜âˆ£ğ‘¥(ğ‘–)):=ğ‘(ğ‘¦=ğ‘˜âˆ£ğ‘¥(ğ‘–);ğœƒ0), for ğ‘˜=1,2, and ğ‘–=0,â€¦,4. 
 
M-step: maximize the expected log-likelihood

ğ‘™Ìƒ (ğ·;ğœƒ):=âˆ‘ğ‘–âˆ‘ğ‘˜ğ‘(ğ‘¦=ğ‘˜âˆ£ğ‘¥(ğ‘–))logğ‘(ğ‘¥(ğ‘–),ğ‘¦=ğ‘˜;ğœƒ)ğ‘(ğ‘¦=ğ‘˜âˆ£ğ‘¥(ğ‘–)) 
 
with respect to  ğœƒ  while keeping  ğ‘(ğ‘¦=ğ‘˜âˆ£ğ‘¥(ğ‘–))  fixed.

To see why this optimizes a lower bound, consider the following inequality:

 	 logğ‘(ğ‘¥;ğœƒ) 	 =logâˆ‘ğ‘¦ğ‘(ğ‘¥,ğ‘¦;ğœƒ) 	 	 
 	 	 =logâˆ‘ğ‘¦ğ‘(ğ‘¦|ğ‘¥)ğ‘(ğ‘¥,ğ‘¦;ğœƒ)ğ‘(ğ‘¦|ğ‘¥) 	 	 
 	 	 =logğ”¼ğ‘¦âˆ¼ğ‘(ğ‘¦|ğ‘¥)[ğ‘(ğ‘¥,ğ‘¦;ğœƒ)ğ‘(ğ‘¦|ğ‘¥)] 	 	 
 	 	 â‰¥ğ”¼ğ‘¦âˆ¼ğ‘(ğ‘¦|ğ‘¥)[logğ‘(ğ‘¥,ğ‘¦;ğœƒ)ğ‘(ğ‘¦|ğ‘¥)] 	 	 
 	 	 =âˆ‘ğ‘¦ğ‘(ğ‘¦|ğ‘¥)logğ‘(ğ‘¥,ğ‘¦;ğœƒ)ğ‘(ğ‘¦|ğ‘¥) 	 	 
where the inequality comes from Jensen's inequality . EM makes this bound tight for the current setting of  ğœƒ  by setting  ğ‘(ğ‘¦|ğ‘¥)  to be  ğ‘(ğ‘¦âˆ£ğ‘¥;ğœƒ0) .

Note: If you have taken 6.431x Probabilityâ€“The Science of Uncertainty, you could review the video in Unit 8: Limit Theorems and Classical Statistics, Additional Theoretical Material, 2. Jensen's Inequality.


## Likelihood Function

What is the log-likelihood of the data  ğ‘™(îˆ°;ğœƒ)  given the initial setting of  ğœƒ ? Please round to the nearest tenth.

Note: You will want to write a script to calculate this, using the natural log (np.log) and np.float64 data types.


Answer = -24.5


## E-Step


What is the formula for  ğ‘(ğ‘¦=ğ‘˜âˆ£ğ‘¥,ğœƒ) ? Write in terms of  ğœ‹ğ‘˜ ,  ğœ‹1 ,  ğœ‹2 ,  ğ‘ğ‘˜ ,  ğ‘1 , and  ğ‘2  (where  ğ‘ğ‘˜=îˆº(ğ‘¥âˆ£ğœ‡ğ‘˜,ğœ2ğ‘˜) ).


Answer = pi_k * N_k) / (pi_1 * N_1 + pi_2 * N_2)
 


## E-Step Weights


For each of the given data points say which Gaussian (1 or 2) they are given more weight towards in the first E-step using the given setting of  ğœƒ0 . This is, answer 2 if  ğ‘(ğ‘¦=2âˆ£ğ‘¥,ğœƒ0)>ğ‘(ğ‘¦=1âˆ£ğ‘¥,ğœƒ0)  and 1 otherwise.


Answer = 2

Answer = 2

Answer = 2

Answer = 1

Answer = 1




## M-Step


Fixing  ğ‘(ğ‘¦=ğ‘˜âˆ£ğ‘¥,ğœƒ0) , we want to update  ğœƒ  such that our lower bound is maximized.

What is the optimal  ğœ‡Ì‚ ğ‘˜ ? For simplicity, assume we only have two data points  ğ‘¥(1)  and  ğ‘¥(2)  for this particular question. Answer in terms of  ğ‘¥(1) ,  ğ‘¥(2) , and  ğ›¾ğ‘˜1 ,  ğ›¾ğ‘˜2 , which are defined to be  ğ›¾ğ‘˜ğ‘–=ğ‘(ğ‘¦=ğ‘˜âˆ£ğ‘¥(ğ‘–);ğœƒ0) 

(For ease of input, use subscripts instead superscripts, i.e. type x_i for  ğ‘¥(ğ‘–) . Type gamma_ki for  ğ›¾ğ‘˜ğ‘– .)


Answer = (gamma_k1 * x_1 + gamma_k2 * x_2) / (gamma_k1 + gamma_k2)

What is the optimal  ğœÌ‚ 2ğ‘˜ ? Answer in terms of  ğ‘¥(1) ,  ğ‘¥(2) ,  ğ›¾ğ‘˜1  and  ğ›¾ğ‘˜2 , which are defined as above to be  ğ›¾ğ‘˜ğ‘–=ğ‘(ğ‘¦=ğ‘˜âˆ£ğ‘¥(ğ‘–);ğœƒ0) , and  ğœ‡Ì‚ ğ‘˜ .

(Type hatmu_k for  ğœ‡Ì‚ ğ‘˜ . As above, for ease of input, use subscripts instead superscripts, i.e. type x_i for  ğ‘¥(ğ‘–) . Type gamma_ki for  ğ›¾ğ‘˜ğ‘– .)


Answer = (gamma_k1 * (x_1 - hatmu_k)^2 + gamma_k2 * (x_2 - hatmu_k)^2) / (gamma_k1 + gamma_k2)


What is the optimal ğœ‹Ì‚ ğ‘˜? Answer in terms of ğ›¾ğ‘˜1 and ğ›¾ğ‘˜2, which are defined as above to be ğ›¾ğ‘˜ğ‘–=ğ‘(ğ‘¦=ğ‘˜âˆ£ğ‘¥(ğ‘–);ğœƒ0),

(As above, type gamma_ki for ğ›¾ğ‘˜ğ‘–.)

Note: that you must account for the constraint that ğœ‹1+ğœ‹2=1 where ğœ‹1,ğœ‹2â‰¥0.

Note: If you know that some aspect of your formula equals an exact constant, simplify and use this number, i.e. ğ›¾11+ğ›¾21=1.


Answer = (gamma_k1 + gamma_k2) / 2
 
## Training 1


In the first M-step, which Gaussian will shift to the left more (relatively)?

Answer = Gaussian 2



## Training 2


In the first M-step, which Gaussian's variance will increase more (relatively)?

Answer = Gaussian 2


## Training 3


After convergence, which variance will be larger?

Answer =  ğœ21



