#Â Problem set 4

## Stochastic Gradient Descent

### SGD and Hinge Loss


As we saw in the lecture above,

ğ½(ğœƒ,ğœƒ0)=1ğ‘›âˆ‘ğ‘–=1ğ‘›Lossâ„(ğ‘¦(ğ‘–)(ğœƒâ‹…ğ‘¥(ğ‘–)+ğœƒ0))+ğœ†2âˆ£âˆ£ğœƒâˆ£âˆ£2=1ğ‘›âˆ‘ğ‘–=1ğ‘›[Lossâ„(ğ‘¦(ğ‘–)(ğœƒâ‹…ğ‘¥(ğ‘–)+ğœƒ0))+ğœ†2âˆ£âˆ£ğœƒâˆ£âˆ£2] 
 
With stochastic gradient descent, we choose  ğ‘–âˆˆ{1,...,ğ‘›}  at random and update  ğœƒ  such that

ğœƒâ†ğœƒâˆ’ğœ‚âˆ‡ğœƒ[Lossâ„(ğ‘¦(ğ‘–)(ğœƒâ‹…ğ‘¥(ğ‘–)+ğœƒ0))+ğœ†2âˆ£âˆ£ğœƒâˆ£âˆ£2] 
 
What is  âˆ‡ğœƒ[Lossâ„(ğ‘¦(ğ‘–)(ğœƒâ‹…ğ‘¥(ğ‘–)+ğœƒ0))]  if  Lossâ„(ğ‘¦(ğ‘–)(ğœƒâ‹…ğ‘¥(ğ‘–)+ğœƒ0))>0 ?


Answer = -yâ½â±â¾xâ½â±â¾


#### Comparison with Perceptron

Observing the update step of SGD,

ğœƒâ†ğœƒâˆ’ğœ‚âˆ‡ğœƒ[Lossâ„(ğ‘¦(ğ‘–)(ğœƒâ‹…ğ‘¥(ğ‘–)+ğœƒ0))+ğœ†2âˆ£âˆ£ğœƒâˆ£âˆ£2] 
 
Which of the following is true?


Answer = Differently from perceptron,  ğœƒ  is updated even when there is no mistake


We can see from

ğœƒâ†{(1âˆ’ğœ†ğœ‚)ğœƒ if Loss=0(1âˆ’ğœ†ğœ‚)ğœƒ+ğœ‚ğ‘¦(ğ‘–)ğ‘¥(ğ‘–) if Loss>0 
 
that  ğœƒ  is updated even when the sum of losses is  0 . This is different from perceptron.
