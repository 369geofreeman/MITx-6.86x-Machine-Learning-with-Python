#Â Homework 2 : Perceptron Performance

In class we initialized the perceptron algorithm with  ğœƒ=0 . In this problem we will also explore other initialization choices.

<hr />

## 2) a.

The following table shows a data set and the number of times each point is misclassified during a run of the perceptron algorithm (with offset  ğœƒ0 ).  ğœƒ  and  ğœƒ0  are initialized to zero.

| ğ‘– | ğ‘¥ (ğ‘–)    | ğ‘¦ (ğ‘–) | times misclassified |
|---|----------|-------|:-------------------:|
| 1 | [-4, 2]  | +1    |          1          |
| 2 | [-2, 1]  | +1    |          0          |
| 3 | [-1, -1] | -1    |          2          |
| 4 | [2, 2]   | -1    |          1          |
| 5 | [1, -2]  | -1    |          0          |


Write down the state of  ğœƒ  and  ğœƒ0  after this run has completed (note, the algorithm may not yet have converged). Enter  ğœƒ  as a list  [ğœƒ1,ğœƒ2]  and  ğœƒ0  as a single number in the following boxes.

Please enter  ğœƒ :

Answer = [-4,2]

Please enter  ğœƒ0  :

Answer = -3


## 2) b

Provide one example of a different initialization of  ğœƒ  such that the perceptron algorithm with this initialization would not produce any mistakes during a run through the data.


Answer = 
[ğœƒ1,ğœƒ2] = 
ğœƒ0 : = 



## 2) c

The theorem from question 1. (e) provides an upper bound on the number of steps of the Perceptron algorithm and implies that it indeed converges. In this question, we will show that the result still holds even when  ğœƒ  is not initialized to  0. 

In other words: Given a set of training examples that are linearly separable through the origin, show that the initialization of  ğœƒ  does not impact the perceptron algorithm's ability to eventually converge.

To derive the bounds for convergence, we assume the following inequalities holds:

There exists  ğœƒâˆ—  such that  ğ‘¦(ğ‘–)(ğœƒâˆ—ğ‘¥(ğ‘–))â€–ğœƒâˆ—â€–â‰¥ğ›¾  for all  ğ‘–=1,â‹¯,ğ‘›  and some  ğ›¾>0 

All the examples are bounded  â€–ğ‘¥(ğ‘–)â€–â‰¤ğ‘…,ğ‘–=1,â‹¯,ğ‘› 

If  ğœƒ  is initialized to  0 , we can show by induction that:

ğœƒ(ğ‘˜)â‹…ğœƒâˆ—â€–ğœƒâˆ—â€–â‰¥ğ‘˜ğ›¾ 
 
For instance,

ğœƒ(ğ‘˜+1)â‹…ğœƒâˆ—â€–ğœƒâˆ—â€–=(ğœƒ(ğ‘˜)+ ğ‘¦(ğ‘–)ğ‘¥(ğ‘–))â‹…ğœƒâˆ—â€–ğœƒâˆ—â€–â‰¥(ğ‘˜+1)ğ›¾ 
 
If we initialize  ğœƒ  to a general (not necessarily 0)  ğœƒ(0) , then:

ğœƒ(ğ‘˜)â‹…ğœƒâˆ—â€–ğœƒâˆ—â€–â‰¥ğ‘+ğ‘˜ğ›¾ 
 
Determine the formulation of  ğ‘  in terms of  ğœƒâˆ—  and  ğœƒ(0) :

Important: Please enter  ğœƒâˆ—  as theta^{star} and  ğœƒ(0)  as theta^{0}, and use norm(...) for the vector norm  â€–...â€– .

Answer = ğ‘= 


If  ğœƒ  is initialized to  0 , we can show by induction that:

â€–ğœƒ(ğ‘˜)â€–2â‰¤ğ‘˜ğ‘…2 
 
For instance,

â€–ğœƒ(ğ‘˜+1)â€–2â‰¤â€–ğœƒ(ğ‘˜)+ğ‘¦(ğ‘–)ğ‘¥(ğ‘–)â€–2â‰¤â€–ğœƒ(ğ‘˜)â€–2+ğ‘…2 
 
If we initialize  ğœƒ  to a general (not necessarily 0)  ğœƒ(0) , then:

â€–ğœƒ(ğ‘˜)â€–2â‰¤ğ‘˜ğ‘…2+ğ‘2 
 
Determine the formulation of  ğ‘2  in terms of  ğœƒ(0) :


Answer = ğ‘2= 

From the above inequality, we can derive the inequality  â€–ğœƒ(ğ‘˜)â€–â‰¤ğ‘+ğ‘˜â€¾â€¾âˆšğ‘…  by applying the following inequality:  ğ‘¥2+ğ‘¦2â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆšâ‰¤(ğ‘¥+ğ‘¦)2â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš  if  ğ‘¥,ğ‘¦>0 .

If  ğœƒ  is initialized to  0 , we then use the fact that  1â‰¥ğœƒ(ğ‘˜)â€–ğœƒ(ğ‘˜)â€–â‹…ğœƒâˆ—â€–ğœƒâˆ—â€–  to get the upper bound  ğ‘˜â‰¤ğ‘…2ğ›¾2 .

In the case where we initialize  ğœƒ  to a general  ğœƒ(0) , use the inequality for  ğœƒ(ğ‘˜)â‹…ğœƒâˆ—â€–ğœƒâˆ—â€–  above and the inequality  â€–ğœƒ(ğ‘˜)â€–â‰¤ğ‘+ğ‘˜â€¾â€¾âˆšğ‘…  to derive a bound on the number of iterations  ğ‘˜ .

Hint: Use the larger root of a quadratic equation to obtain the upper bound.

Note: Give your answer in terms of  ğ‘,ğ‘,ğ‘…,ğ›¾  (enter the latter as gamma).


Answer = ğ‘˜ â‰¤ 

## 2) d

Since the convergence of the perceptron algorithm doesn't depend on the initialization, the end performance on the training set must be the same. Are the resulting  ğœƒ 's the same regardless of the initialization?

Answer = No

Does this necessarily imply that the performance on a test set is the same?

Answer = No
