# RL Terminology


A Markov decision process (MDP) is defined by

a set of states  ğ‘ âˆˆğ‘† ;

a set of actions  ğ‘âˆˆğ´ ;

Action dependent transition probabilities  ğ‘‡(ğ‘ ,ğ‘,ğ‘ â€²)=ğ‘ƒ(ğ‘ â€²|ğ‘ ,ğ‘) , so that for each state  ğ‘   and action  ğ‘ ,  âˆ‘ğ‘ â€²âˆˆğ‘†ğ‘‡(ğ‘ ,ğ‘,ğ‘ â€²)=1 .

Reward functions  ğ‘…(ğ‘ ,ğ‘,ğ‘ â€²),  representing the reward for starting in state  ğ‘  , taking action  ğ‘  and ending up in state  ğ‘ â€²  after one step. (The reward function may also depend only on  ğ‘  , or only  ğ‘   and  ğ‘ .)

MDPs satisfy the Markov property in that the transition probabilities and rewards depend only on the current state and action, and remain unchanged regardless of the history (i.e. past states and actions) that leads to the current state.

Note: If you have taken 6.431x Probabilityâ€“The Science of Uncertainty and Data, you may review the Markov Property and Markov Chains introduced in Unit 10 Markov Processes. Markov decision processes are extensions of Markov Chains by the set of actions (and the action-dependence of the transition probabilities), and the reward functions.


## Markov Property

Let  ğ‘‹ğ‘–,ğ‘–=1,2,â€¦  be a discrete Markov chain with states  {ğ‘ ğ‘—,ğ‘—âˆˆâ„•} . Which of the following statements are correct?


Answer = For  ğ‘›â‰¥3 ,  ğ‘ƒ[ğ‘‹ğ‘›=ğ‘¥ğ‘›âˆ£ğ‘‹ğ‘›âˆ’1=ğ‘¥ğ‘›âˆ’1,ğ‘‹1=ğ‘¥1]=ğ‘ƒ[ğ‘‹ğ‘›=ğ‘¥ğ‘›âˆ£ğ‘‹ğ‘›âˆ’1=ğ‘¥ğ‘›âˆ’1].

Answer = For  ğ‘›â‰¥3  and  ğ‘›âˆ’ğ‘—>1 ,  ğ‘ƒ[ğ‘‹ğ‘›=ğ‘¥ğ‘›âˆ£ğ‘‹ğ‘›âˆ’ğ‘—=ğ‘¥ğ‘›âˆ’ğ‘—,ğ‘‹1=ğ‘¥1]=ğ‘ƒ[ğ‘‹ğ‘›=ğ‘¥ğ‘›âˆ£ğ‘‹ğ‘›âˆ’ğ‘—=ğ‘¥ğ‘›âˆ’ğ‘—].



An AI agent navigates in the 3x3 grid depicted above, where the middle square is not accessible by the agent (and hence is greyed out).

The MDP is defined as follows:

Every state  ğ‘   is defined by the current position of the agent in the grid (and is independent of its previous actions and positions).

The actions  ğ‘  are the 4 directions â€œup", â€œdown",â€œleft", â€œright".

The transition probabilities from state  ğ‘   via action  ğ‘  to state  ğ‘ â€²  is given by  ğ‘‡(ğ‘ ,ğ‘,ğ‘ â€²)=ğ‘ƒ(ğ‘ â€²|ğ‘ ,ğ‘) .

The agent receives a reward of  +1  for arriving at the top right cell, and a reward of  âˆ’1  for arriving in the cell immediately below it. It does not receive any non-zero reward at the other cells as illustrated in the following figure.


## Markovian Setting

Let  ğ‘   be any given state in this MDP. The agent takes actions  ğ‘1,ğ‘2â€¦ğ‘ğ‘›  starting from state  ğ‘ 0  and as a result visits states  ğ‘ 1,ğ‘ 2â€¦ğ‘ ğ‘›=ğ‘   in that order.

Given that  ğ‘ ğ‘›=ğ‘ ,  that is, the agent ends up at the current state  ğ‘   after  ğ‘›  steps, what do the rewards after the  ğ‘›th  step depend on? (Choose all that apply.)


Answer = Rewards collected after the  ğ‘›th  step do not depend on the previous states  ğ‘ 1,ğ‘ 2â€¦ğ‘ ğ‘›âˆ’1  correct

Answer = Rewards collected after the  ğ‘›th  step can depend on the current state  ğ‘   correct

Answer = Rewards collected after the  ğ‘›th  step do not depend on the previous actions  ğ‘1,ğ‘2â€¦ğ‘ğ‘›  


## Number of States


Enter the total number of unique states in the MDP described above and depicted by the  3Ã—3  grid above. (Enter  âˆ’1  if the number of states is not finite.)


Answer = 8


## Transition Probabilities

Refer to the MDP described and depicted in the  3Ã—3  grid on the top of this page.

Assume that the transition probabilities for all the states are given as a table  ğ‘€ , whose  (ğ‘–,ğ‘—,ğ‘˜) -th entry is  ğ‘€[ğ‘–][ğ‘—][ğ‘˜]=ğ‘‡(ğ‘ ğ‘–,ğ‘ğ‘—,ğ‘ ğ‘˜)=ğ‘ƒ(ğ‘ ğ‘˜|ğ‘ ğ‘–,ğ‘ğ‘—),  which represents the transition probability of ending up at state  ğ‘ ğ‘˜  when action  ğ‘ğ‘—  is taken from the state  ğ‘ ğ‘– . Note: Note that here,  ğ‘ ğ‘–  and  ğ‘ ğ‘˜  can be any pair of states, not necessarily reachable by an action in one step.

Enter the number of entries in the table  ğ‘€ :


Answer = 256


