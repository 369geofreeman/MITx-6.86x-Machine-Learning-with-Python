# Policy and Value Functions



## Definition of Optimal Policy

Given an MDP, and a utility function  ğ‘ˆ[ğ‘ 0,ğ‘ 1,â€¦,ğ‘ ğ‘›], , our goal is to find an optimal policy function that maximizes the expectation of the utility. Here, a policy is a function  ğœ‹:ğ‘†â†’ğ´  that assigns an action  ğœ‹(ğ‘ )  to any state  ğ‘  . We denote the optimal policy by  ğœ‹âˆ— .

Which of the following option is correct about the optimal policy function?



Answer = The  optimal policy assigns an action at every state that maximizes the expected utility.


## Optimal policy - Numerical Example


Recall that in this setup, the agent receives a reward (or penalty) of  âˆ’10  for every action that it takes, on top of the  +1  and  âˆ’1  when it reached the corresponding cells. Since the agent always starts at the state  ğ‘ 0 , and the outcome of each action is deterministic, the discounted reward depends only on the action sequences and can be written as:

 	 ğ‘ˆ[ğ‘1,ğ‘2,â€¦] 	 = 	 ğ‘…(ğ‘ 0,ğ‘1)+ğ›¾ğ‘…(ğ‘ 1,ğ‘2)+ğ›¾2ğ‘…(ğ‘ 2,ğ‘3)+â‹¯ 	 	 
where the sum is until the agent stops.

For the cases  ğ›¾=0  and  ğ›¾=0.5 , what is the maximum discounted reward that the agent can accumulate by starting at the bottom right corner and taking actions until it reached the top right corner?

(Remember the negative reward  âˆ’10  is applied to any action taken, including one that leads the agent to the  âˆ’1  or  +1  cells.)

For  ğ›¾=0 :


Answer = 


Answer = 



## Value Function

As above, we are working with the  3Ã—3  grid example with  +1  reward at the top right corner and  âˆ’1  at the cell below it. The agent also gets a reward of  âˆ’10  for every action that it takes. The action outcomes are deterministic. The agent continues to act until it reaches the  +1  cell, when it stops.

The following figures show states  ğ‘ 1,ğ‘ 2,ğ‘ 3,  in which the letter â€œA" marks the current location of the agent.

		
A value function  ğ‘‰(ğ‘ )  of a given state  ğ‘   is the expected reward (i.e the expectation of the utility function) if the agent acts optimally starting at state  ğ‘  . In the given MDP, since the action outcome is deterministic, the expected reward simply equals the utility function.

Which of the following should hold true for a good value function  ğ‘‰(ğ‘ )  under the reward structure in the given MDP?

Note: You may want to watch the video on the next page before submitting this question.


Answer = ğ‘‰(ğ‘ 3)<ğ‘‰(ğ‘ 1)<ğ‘‰(ğ‘ 2)




