# Q-value Iteration


The derivation of the Q-value iteration update rule from the equation above is similar to the derivation of the value iteration update rule.

First, recall the Bellman equations:

 	 ğ‘‰âˆ—(ğ‘ ) 	 = 	 maxğ‘ğ‘„âˆ—(ğ‘ ,ğ‘) 	 	 
 	 ğ‘„âˆ—(ğ‘ ,ğ‘) 	 = 	 âˆ‘ğ‘ â€²ğ‘‡(ğ‘ ,ğ‘,ğ‘ â€²)(ğ‘…(ğ‘ ,ğ‘,ğ‘ â€²)+ğ›¾ğ‘‰âˆ—(ğ‘ â€²)). 	 	 
Plugging first equation into the second, we get:

 	 ğ‘„âˆ—(ğ‘ ,ğ‘) 	 = 	 âˆ‘ğ‘ â€²ğ‘‡(ğ‘ ,ğ‘,ğ‘ â€²)(ğ‘…(ğ‘ ,ğ‘,ğ‘ â€²)+ğ›¾maxğ‘â€²ğ‘„âˆ—(ğ‘ â€²,ğ‘â€²)). 	 	 
Now, let  ğ‘„âˆ—ğ‘˜(ğ‘ ,ğ‘)  be the expected rewards from state  ğ‘   followed by action  ğ‘ , and then acting optimally for  ğ‘˜  steps afterwards. (Hence,  ğ‘‰âˆ—ğ‘˜(ğ‘ )=maxğ‘ğ‘„âˆ—ğ‘˜(ğ‘ ,ğ‘) .)


## Q-Value Iteration Update Rule


Referring to the equations above, what should the Q-value iteration update rule be?

Answer = ğ‘„âˆ—ğ‘˜+1(ğ‘ ,ğ‘)=âˆ‘ğ‘ â€²ğ‘‡(ğ‘ ,ğ‘,ğ‘ â€²)(ğ‘…(ğ‘ ,ğ‘,ğ‘ â€²)+ğ›¾maxğ‘â€²ğ‘„âˆ—ğ‘˜(ğ‘ â€²,ğ‘â€²))



