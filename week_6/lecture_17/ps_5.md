# Bellman Equations



Recall from lecture the Bellman Equations are

 	 ğ‘‰âˆ—(ğ‘ ) 	 = 	 maxğ‘ğ‘„âˆ—(ğ‘ ,ğ‘) 	 	 
 	 ğ‘„âˆ—(ğ‘ ,ğ‘) 	 = 	 âˆ‘ğ‘ â€²ğ‘‡(ğ‘ ,ğ‘,ğ‘ â€²)(ğ‘…(ğ‘ ,ğ‘,ğ‘ â€²)+ğ›¾ğ‘‰âˆ—(ğ‘ â€²)) 	 	 
where

the value function  ğ‘‰âˆ—(ğ‘ )  is the expected reward from starting at state  ğ‘   and acting optimally.

the Q-function  ğ‘„âˆ—(ğ‘ ,ğ‘)  is the expected reward from starting at state  ğ‘  , then acting with action  ğ‘ , and acting optimally afterwards.


## Value Function in Terms of Q Function

Let us work through a numerical example to understand the Bellman equations.

Let there be  4  possible actions,  ğ‘1,ğ‘2,ğ‘3,ğ‘4,  from a given state  ğ‘  , and let the  ğ‘„âˆ—  values be as follows:

 	 ğ‘„âˆ—(ğ‘ ,ğ‘1) 	 = 	 10 	 	 
 	 ğ‘„âˆ—(ğ‘ ,ğ‘2) 	 = 	 âˆ’1 	 	 
 	 ğ‘„âˆ—(ğ‘ ,ğ‘3) 	 = 	 0 	 	 
 	 ğ‘„âˆ—(ğ‘ ,ğ‘4) 	 = 	 11. 	 	 
Enter the value of  ğ‘‰âˆ—(ğ‘ )  below:


Answer = 11


## Bellman Equation for Q Function

As above, let there be  4  possible actions,  ğ‘1,ğ‘2,ğ‘3,ğ‘4,  from a given state  ğ‘   wth  ğ‘„âˆ—  values given below:

 	 ğ‘„âˆ—(ğ‘ ,ğ‘1) 	 = 	 10 	 	 
 	 ğ‘„âˆ—(ğ‘ ,ğ‘2) 	 = 	 âˆ’1 	 	 
 	 ğ‘„âˆ—(ğ‘ ,ğ‘3) 	 = 	 0 	 	 
 	 ğ‘„âˆ—(ğ‘ ,ğ‘4) 	 = 	 11. 	 	 
Let  ğ‘ â€²  be a state that can be reached from  ğ‘   by taking the action  ğ‘1 . Let

 	 ğ‘‡(ğ‘ ,ğ‘1,ğ‘ â€²) 	 = 	 1 	 	 
 	 ğ‘…(ğ‘ ,ğ‘1,ğ‘ â€²) 	 = 	 5 	 	 
 	 ğ›¾ 	 = 	 0.5. 	 	 
Enter the value of  ğ‘‰âˆ—(ğ‘ â€²)  below:


Answer = 10



