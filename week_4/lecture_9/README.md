# Lecture 9. Feedforward Neural Networks, Back Propagation, and Stochastic Gradient Descent (SGD)

* Feedforward Neural Networks, Back-propagation, and Stochastic Gradient Descent At the end of this lecture, you will be able to

* Write down recursive relations with back-propagation algorithm to compute the gradient of the loss function with respect to the weight parameters.

* Use the stochastic descent algorithm to train a feedforward neural network.

* Understand that it is not guaranteed to reach global (only local) optimum with SGD to minimize the training loss.

* Recognize when a network has overcapacity .
