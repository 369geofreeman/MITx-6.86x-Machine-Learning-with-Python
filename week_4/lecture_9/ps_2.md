# Problem set 2


## Training Models with 1 Hidden Layer


### Training Models with 1 Hidden Layer, Overcapacity, and Convergence Guarantees


**SGD Convergence guarantees**

Which of the following option(s) is/are true about training neural networks?
(Choose all that apply.)

Answer = For multi-layer neural networks, stochastic gradient descent (SGD) is not guaranteed to reach a global optimum

Answer = Larger models tend to be easier to learn because their units need to be adjusted so that they are, collectively sufficient to solve the task


