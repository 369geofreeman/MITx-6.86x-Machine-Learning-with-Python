#Â Problem set 1


### Back-propagation Algorithm


Once we set up the architecture of our (feedforward) neural network, our goal will be to find weight parameters that minimize our loss function. We will use the stochastic gradient descent algorithm (which you learned in Lecture 4 and revisited in lecture 5) to carry out the optimization.

This involves computing the gradient of the loss function with respect to the weight parameters.

Since the loss function is a long chain of compositions of activation functions with the weight parameters entering at different stages, we will break down the computation of the gradient into different pieces via the chain rule; this way of computing the gradient is called the back-propagation algorithm.

In the following problems, we will explore the main step in the stochastic gradient descent algorithm for training the following simple neural network from the video:


This simple neural network is made up of  ğ¿ hidden layers, but each layer consists of only one unit, and each unit has activation function  ğ‘“ .
As usual,  ğ‘¥  is the input,  ğ‘§ğ‘–  is the weighted combination of the inputs to the  ğ‘–ğ‘¡â„  hidden layer. In this one-dimensional case, weighted combination reduces to products:

 	 ğ‘§1 	 = 	 ğ‘¥ğ‘¤1 	 	 
 	 for ğ‘–=2â€¦ğ¿:ğ‘§ğ‘– 	 = 	 ğ‘“ğ‘–âˆ’1ğ‘¤ğ‘–where ğ‘“ğ‘–âˆ’1=ğ‘“(ğ‘§ğ‘–âˆ’1). 	 	 
We will use the following loss function:

 	 îˆ¸(ğ‘¦,ğ‘“ğ¿)=(ğ‘¦âˆ’ğ‘“ğ¿)2 	 	 
where  ğ‘¦  is the true value, and  ğ‘“ğ¿  is the output of the neural network.


### Gradient Descent Update

Let  ğœ‚  be the learning rate for the stochastic gradient descent algorithm.

Recall that our goal is to tune the parameters of the neural network so as to minimize the loss function. Which of the following is the appropriate update rule for the paramter  ğ‘¤1  in the stochastic gradient descent algorithm?


Answer =  ğ‘¤1â†ğ‘¤1âˆ’ğœ‚â‹…âˆ‡ğ‘¤1îˆ¸(ğ‘¦,ğ‘“ğ¿)


### Recursive Expression - Part I


As above, let  îˆ¸(ğ‘¦,ğ‘“ğ¿)  denote the loss function as a function of the predictions  ğ‘“ğ¿  and the true label  ğ‘¦ . Recall

 	 ğ‘§1 	 = 	 ğ‘¥ğ‘¤1 	 	 
 	 for ğ‘–=2â€¦ğ¿:ğ‘§ğ‘– 	 = 	 ğ‘“ğ‘–âˆ’1ğ‘¤ğ‘–where ğ‘“ğ‘–âˆ’1=ğ‘“(ğ‘§ğ‘–âˆ’1). 	 	 
Let  ğ›¿ğ‘–=âˆ‚îˆ¸âˆ‚ğ‘§ğ‘– .

The first step to updating any weight  ğ‘¤  is to calculate  âˆ‚îˆ¸âˆ‚ğ‘¤ .

Which of the following option(s) is/are correct expression(s) for  âˆ‚îˆ¸âˆ‚ğ‘¤1 ?
(Choose all that apply.)

Answer = âˆ‚îˆ¸âˆ‚ğ‘¤1=âˆ‚ğ‘§1âˆ‚ğ‘¤1â‹…âˆ‚îˆ¸âˆ‚ğ‘§1
Answer = âˆ‚îˆ¸âˆ‚ğ‘¤1=ğ‘¥â‹…ğ›¿1



### Recursive Expression - Part II

As above, let  îˆ¸(ğ‘¦,ğ‘“ğ¿)  denote the loss function as a function of the predictions  ğ‘“ğ¿  and the true label  ğ‘¦.  Let  ğ›¿ğ‘–=âˆ‚îˆ¸âˆ‚ğ‘§ğ‘– .

In this problem, we derive a recurrence relation between  ğ›¿ğ‘–  and  ğ›¿ğ‘–+1 

Assume that  ğ‘“  is the hyperbolic tangent function:

 	 ğ‘“(ğ‘¥) 	 = 	 tanh(ğ‘¥) 	 	 
 	 ğ‘“â€²(ğ‘¥) 	 = 	 (1âˆ’tanh2(ğ‘¥)). 	 	 
Which of the following option is the correct expression for  ğ›¿1  in terms of  ğ›¿2 ?

Answer =  ğ›¿1=(1âˆ’ğ‘“21)â‹…ğ‘¤2â‹…ğ›¿2

### Final Expression of the Gradient

As above, let  îˆ¸(ğ‘¦,ğ‘“ğ¿)  denote the loss function as a function of the predictions  ğ‘“ğ¿  and the true label  ğ‘¦.  Let  ğ›¿ğ‘–=âˆ‚îˆ¸âˆ‚ğ‘§ğ‘– .

In this problem, we unroll the recurrence expression for  âˆ‚îˆ¸âˆ‚ğ‘¤1 . We will use the loss function

 	 îˆ¸(ğ‘¦,ğ‘“ğ¿)=(ğ‘¦âˆ’ğ‘“ğ¿)2. 	 	 
Compute  âˆ‚îˆ¸âˆ‚ğ‘¤1  and select the correct option from below.


Answer = âˆ‚îˆ¸âˆ‚ğ‘¤1=ğ‘¥(1âˆ’ğ‘“21)(1âˆ’ğ‘“22)â‹¯(1âˆ’ğ‘“2ğ¿)ğ‘¤2ğ‘¤3â‹¯ğ‘¤ğ¿(2(ğ‘“ğ¿âˆ’ğ‘¦))

