#Â Problem set one

### Motivation

Consider the classification decision rule

ğ‘¦Ì‚ =sign(ğœƒâ‹…ğœ™(ğ‘¥)) 
 
where  ğ‘¥âˆˆâ„ğ‘‘  represent input data and  ğ‘¦âˆˆ{1,âˆ’1}  is the corresponding predicted labels, and we have omitted the bias/offset term for simplicity.

Given the model above, determine if the following statements are True or False.

The feature map  ğœ™  is function from  â„ğ‘‘  to  â„ğ‘‘ .

Answer = false, because its not neccessary for the feature map to be a function from Rd to Rd ,it depends on the dimension of theta.

If  ğœ™(ğ‘¥)âˆˆâ„ğ·,  then the classification parameter  ğœƒ  is also a vector in  â„ğ· . (Answer based on the model as written.)


Answer = true, if feature map is a function from Rd to Rd ,then theta must be also in Rd , else the dot product is not possible between them.


### Neural networks

Motivation to Neural Networks

So far, the ways we have performed non-linear classification involve either first mapping  ğ‘¥  explicitly into some feature vectors  ğœ™(ğ‘¥),  whose coordinates involve non-linear functions of  ğ‘¥,  or in order to increase computational efficiency, rewriting the decision rule in terms of a chosen kernel, i.e. the dot product of feature vectors, and then using the training data to learn a transformed classification parameter.

However, in both cases, the feature vectors are chosen . They are not learned in order to improve performance of the classification problem at hand.

Neural networks, on the other hand, are models in which the feature representation is learned jointly with the classifier to improve classification performance.

