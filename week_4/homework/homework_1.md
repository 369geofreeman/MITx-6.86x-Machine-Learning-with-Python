#Â Nerual Networks


In this problem we will analyze a simple neural network to understand its classification properties. Consider the neural network given in the figure below, with ReLU activation functions (denoted by ğ‘“) on all neurons, and a softmax activation function in the output layer:


Given an input ğ‘¥=[ğ‘¥1,ğ‘¥2]ğ‘‡, the hidden units in the network are activated in stages as described by the following equations:

 	ğ‘§1	=ğ‘¥1ğ‘Š11+ğ‘¥2ğ‘Š21+ğ‘Š01	ğ‘“(ğ‘§1)	=max{ğ‘§1,0}	 	 
 	ğ‘§2	=ğ‘¥1ğ‘Š12+ğ‘¥2ğ‘Š22+ğ‘Š02	ğ‘“(ğ‘§2)	=max{ğ‘§2,0}	 	 
 	ğ‘§3	=ğ‘¥1ğ‘Š13+ğ‘¥2ğ‘Š23+ğ‘Š03	ğ‘“(ğ‘§3)	=max{ğ‘§3,0}	 	 
 	ğ‘§4	=ğ‘¥1ğ‘Š14+ğ‘¥2ğ‘Š24+ğ‘Š04	ğ‘“(ğ‘§4)	=max{ğ‘§4,0}	 	 
 	ğ‘¢1	=ğ‘“(ğ‘§1)ğ‘‰11+ğ‘“(ğ‘§2)ğ‘‰21+ğ‘“(ğ‘§3)ğ‘‰31+ğ‘“(ğ‘§4)ğ‘‰41+ğ‘‰01	ğ‘“(ğ‘¢1)	=max{ğ‘¢1,0}	 	 
 	ğ‘¢2	=ğ‘“(ğ‘§1)ğ‘‰12+ğ‘“(ğ‘§2)ğ‘‰22+ğ‘“(ğ‘§3)ğ‘‰32+ğ‘“(ğ‘§4)ğ‘‰42+ğ‘‰02	ğ‘“(ğ‘¢2)	=max{ğ‘¢2,0}.	 	 
The final output of the network is obtained by applying the softmax function to the last hidden layer,

 	ğ‘œ1	=ğ‘’ğ‘“(ğ‘¢1)ğ‘’ğ‘“(ğ‘¢1)+ğ‘’ğ‘“(ğ‘¢2)	 	 
 	ğ‘œ2	=ğ‘’ğ‘“(ğ‘¢2)ğ‘’ğ‘“(ğ‘¢1)+ğ‘’ğ‘“(ğ‘¢2).	 	 
In this problem, we will consider the following setting of parameters:

â¡â£â¢â¢â¢â¢ğ‘Š11ğ‘Š12ğ‘Š13ğ‘Š14ğ‘Š21ğ‘Š22ğ‘Š23ğ‘Š24ğ‘Š01ğ‘Š02ğ‘Š03ğ‘Š04â¤â¦â¥â¥â¥â¥=â¡â£â¢â¢â¢â¢10âˆ’10010âˆ’1âˆ’1âˆ’1âˆ’1âˆ’1â¤â¦â¥â¥â¥â¥,
 
[ğ‘‰11ğ‘‰12ğ‘‰21ğ‘‰22ğ‘‰31ğ‘‰32ğ‘‰41ğ‘‰42ğ‘‰01ğ‘‰02]=[1âˆ’11âˆ’11âˆ’11âˆ’102].


###  Feed Forward Step


Consider the input  ğ‘¥1=3 ,  ğ‘¥2=14 . What is the final output  (ğ‘œ1,ğ‘œ2)  of the network?

Important: Numerical outputs from the softmax function are sometimes extremely close to 0 or 1. We recommend you enter you answer as a mathematical expression, such as eâˆ§2+1. If you choose to enter your answers as a decimal, you must enter the decimal accurate to at least 9 decimal places .


Answer = 
