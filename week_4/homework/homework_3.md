# Backpropagation


One of the key steps for training multi-layer neural networks is stochastic gradient descent. We will use the back-propagation algorithm to compute the gradient of the loss function with respect to the model parameters.

Consider the  ğ¿ -layer neural network below:


In the following problems, we will the following notation:  ğ‘ğ‘™ğ‘—  is the bias of the  ğ‘—ğ‘¡â„  neuron in the  ğ‘™ğ‘¡â„  layer,  ğ‘ğ‘™ğ‘—  is the activation of  ğ‘—ğ‘¡â„  neuron in the  ğ‘™ğ‘¡â„  layer, and  ğ‘¤ğ‘™ğ‘—ğ‘˜  is the weight for the connection from the  ğ‘˜ğ‘¡â„  neuron in the  (ğ‘™âˆ’1)ğ‘¡â„  layer to the  ğ‘—ğ‘¡â„  neuron in the  ğ‘™ğ‘¡â„  layer.

If the activation function is  ğ‘“  and the loss function we are minimizing is  ğ¶ , then the equations describing the network are:

 	 ğ‘ğ‘™ğ‘— 	 =ğ‘“(âˆ‘ğ‘˜ğ‘¤ğ‘™ğ‘—ğ‘˜ğ‘ğ‘™âˆ’1ğ‘˜+ğ‘ğ‘™ğ‘—) 	 	 
 	 Loss 	 =ğ¶(ğ‘ğ¿) 	 	 
Note that notations without subscript denote the corresponding vector or matrix, so that  ğ‘ğ‘™  is activation vector of the  ğ‘™ğ‘¡â„  layer, and  ğ‘¤ğ‘™  is the weights matrix in  ğ‘™ğ‘¡â„  layer.

For  ğ‘™=1,â€¦,ğ¿ .


### Computing the Error

Let the weighted inputs to the  ğ‘‘  neurons in layer  ğ‘™  be defined as  ğ‘§ğ‘™â‰¡ğ‘¤ğ‘™ğ‘ğ‘™âˆ’1+ğ‘ğ‘™ , where  ğ‘§ğ‘™âˆˆâ„ğ‘‘ . As a result, we can also write the activation of layer  ğ‘™  as  ğ‘ğ‘™â‰¡ğ‘“(ğ‘§ğ‘™) , and the â€œerror" of neuron  ğ‘—  in layer  ğ‘™  as  ğ›¿ğ‘™ğ‘—â‰¡âˆ‚ğ¶âˆ‚ğ‘§ğ‘™ğ‘— . Let  ğ›¿ğ‘™âˆˆâ„ğ‘‘  denote the full vector of errors associated with layer  ğ‘™ .

Back-propagation will give us a way of computing  ğ›¿ğ‘™  for every layer.

Assume there are  ğ‘‘  outputs from the last layer (i.e.  ğ‘ğ¿âˆˆâ„ğ‘‘ ). What is  ğ›¿ğ¿ğ‘—  for the last layer?

Answer = 


What is  ğ›¿ğ‘™ğ‘—  for all  ğ‘™â‰ ğ¿ ?

Answer = âˆ‘ğ‘˜ğ‘¤ğ‘™+1ğ‘˜ğ‘—ğ›¿ğ‘™+1ğ‘˜ğ‘“â€²(ğ‘§ğ‘™ğ‘—)


### Parameter Derivatives

During SGD we are interested in relating the errors computed by back-propagation to the quantities of real interest: the partial derivatives of the loss with respect to our parameters. Here that is  âˆ‚ğ¶âˆ‚ğ‘¤ğ‘™ğ‘—ğ‘˜  and  âˆ‚ğ¶âˆ‚ğ‘ğ‘™ğ‘— .

What is  âˆ‚ğ¶âˆ‚ğ‘¤ğ‘™ğ‘—ğ‘˜ ? Write in terms of the variables  ğ‘ğ‘™âˆ’1ğ‘˜ ,  ğ‘¤ğ‘™ğ‘— ,  ğ‘ğ‘™ğ‘— , and  ğ›¿ğ‘™ğ‘—  if necessary.

Example of writing superscripts and subscripts:

ğ‘‘ğ‘’ğ‘™ğ‘¡ğ‘_ğ‘—\^ğ‘™  for  ğ›¿ğ‘™ğ‘— 

ğ‘¤_(ğ‘—ğ‘˜)\^ğ‘™  for  ğ‘¤ğ‘™ğ‘—ğ‘˜

Answer = a_k^(l-1)**delta_j^l 

What is  âˆ‚ğ¶âˆ‚ğ‘ğ‘™ğ‘— ? Write in terms of the variables  ğ‘ğ‘™âˆ’1ğ‘˜ ,  ğ‘¤ğ‘™ğ‘— ,  ğ‘ğ‘™ğ‘— , and  ğ›¿ğ‘™ğ‘—  if necessary.

Answer = delta_j^l


### Activation Functions: Sigmoid


Recall that there are several different possible choices of activation functions  ğ‘“ . Let's get more familiar with them and their gradients.

What is the derivative of the sigmoid function,  ğœ(ğ‘§)=11+ğ‘’âˆ’ğ‘§ ? Please write your answer in terms of  ğ‘’  and  ğ‘§ :


Answer = 1-(1/(1+e^(-z)))

Which of the following is true of  ğœâ€²(ğ‘§)  as  ||ğ‘§||  gets large?

Answer = Its magnitude becomes small.

What is the derivative of the ReLU function,  ReLU(ğ‘§)=max(0,ğ‘§)  for  ğ‘§>0 ?

Answer = 1

For  ğ‘§<0 ?

Answer = 0

### Simple Network


Consider a simple 2-layer neural network with a single neuron in each layer. The loss function is the quadratic loss:  ğ¶=12(ğ‘¦âˆ’ğ‘¡)2 , where  ğ‘¦  is the prediction and  ğ‘¡  is the target.

Starting with input  ğ‘¥  we have:

ğ‘§1=ğ‘¤1ğ‘¥ 

ğ‘1=ReLU(ğ‘§1) 

ğ‘§2=ğ‘¤2ğ‘1+ğ‘ 

ğ‘¦=ğœ(ğ‘§2) 

ğ¶=12(ğ‘¦âˆ’ğ‘¡)2 

Consider a target value  ğ‘¡=1  and input value  ğ‘¥=3 . The weights and bias are  ğ‘¤1=0.01 ,  ğ‘¤2=âˆ’5 , and  ğ‘=âˆ’1 .

Please provide numerical answers accurate to at least three decimal places.

What is the loss?


Answer = 0.288

What are the derivatives with respect to the parameters?

Answer = 2.080

Answer = -0.004

Answer = -0.1387


### SGD


Referring to the previous problem, what is the update rule for  ğ‘¤1  in the SGD algorithm with step size  ğœ‚ ? Write in terms of  ğ‘¤1 ,  ğœ‚ , and  âˆ‚ğ¶âˆ‚ğ‘¤1 ; enter the latter as (partialC)/(partialw_1), noting the lack of space in the variable names:


Answer next w1 = w_1-eta*(partialC)/(partialw_1)








